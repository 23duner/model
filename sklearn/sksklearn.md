这种笔记 就随便写两句 把遇到的问题写进去

sklearn是机器学习中一个常用的python第三方模块，对常用的机器学习算法进行了封装 其中包括： 1.分类（Classification） 2.回归（Regression） 3.聚类（Clustering） 4.数据降维（Dimensionality reduction） 5.常用模型（Model selection） 6.数据预处理（Preprocessing）
## k聚类
1. "维度诅咒"(Curse of Dimensionality) 写论文用进去不明觉厉
   ```
   K-Means is a clustering algorithm used to group data based on similarities. First, a set number of cluster centroids are instantiated in n-dimensional space, where n corresponds to the number of input variables, in this case the eleven sonic variables. The number of clusters that we instantiate is a hyperparameter that can be tuned. Next, all points are assigned to one of these centroids to form clusters. A point is assigned to its closest centroid according to Euclidean distance. The cluster centroids of these new clusters is determined and the process repeats. The goal of this algorithm in the case of our influence network is to minimize the average sum-of-squarederrors until it converges for two groups of cluster centroids, which is defined by:E = 1|NTot||NTot|∑i=1d(si, R(si))2 (7) where si is the Sonic Point associated with the artist vi ∈ NTot and R(si) is cluster center closest to si [14]. Thus, we seek to minimize the average Euclidean distance of each sonic point to the nearest sonic center. Using scikit-learn, we apply the K-Means clustering algorithm to the entire data set of artists. Hinneburg, Aggarwal, and Keim suggest that the best approach for calculating closeness between the cluster centroids and all other points is Euclidean Distance, so we choose to use that sonic similarity metric here [14]. However, we must be wary of the "Curse of Dimensionality," the issue that when applying certain metrics statistical metrics, the error of the metric increases as the number of dimensions increases. Is eleven dimensions too many and constitutes an issue with using Euclidean Distance? Other research into image classification problems in data science sometimes uses K-Means clustering. For example, Ranjan et al. reduce the number of input dimensions, or Principle Components, to 50 and then apply K-Means clustering [11]. 50 is still more than our 11 dimensions. Furthermore, Hinneburg et al. analytically prove that Euclidean Distance and Manhattan Distance 6 are appropriate indicators of similarity in higher dimensions [7].
   ```